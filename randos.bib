@Book{Cavalli-Sforza:96,
  author =       {Cavalli-Sforza, L. L. and Menozzi, P. and Piazza, A.},
  title =        {The history and geography of human genes},
  publisher =    {Princeton University Press},
  year =         {1996},
  address =      {Princeton, N.J},
  note = {{\small (history and geography are related through PCA)}},
}

@software{annoy,
  author = {Erik Bernhardsson},
  title = {{Annoy (Approximate Nearest Neighbors Oh Yeah)}},
  url = {https://github.com/spotify/annoy},
  version = {2.0.4},
  year = {2015}
}

@ARTICLE{FISTA,
  author={Beck, Amir and Teboulle, Marc},
  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={A fast Iterative Shrinkage-Thresholding Algorithm with application to wavelet-based image deblurring}, 
  year={2009},
  volume={},
  number={},
  pages={693-696},
  doi={10.1109/ICASSP.2009.4959678}}


@ARTICLE{SDSS_DR7,
   author = {{Abazajian}, K.~N. and {Adelman-McCarthy}, J.~K. and {Ag{\"u}eros}, M.~A. and 
  {Allam}, S.~S. and {Allende Prieto}, C. and {An}, D. and {Anderson}, K.~S.~J. and 
  {Anderson}, S.~F. and {Annis}, J. and {Bahcall}, N.~A. and et al.},
    title = "{The Seventh Data Release of the Sloan Digital Sky Survey}",
  journal = {Astrophysical Journal Supplement Series},
archivePrefix = "arXiv",
   eprint = {0812.0649},
 keywords = {atlases, catalogs, surveys},
     year = 2009,
    month = jun,
   volume = 182,
      eid = {543-558},
    pages = {543-558},
      doi = {10.1088/0067-0049/182/2/543},
   adsurl = {http://adsabs.harvard.edu/abs/2009ApJS..182..543A},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@INCOLLECTION{Haufe2009-yt,
  title     = "Estimating vector fields using sparse basis field expansions",
  booktitle = "Advances in Neural Information Processing Systems 21",
  author    = "Haufe, Stefan and Nikulin, Vadim V and Ziehe, Andreas and
               M{\"u}ller, Klaus-Robert and Nolte, Guido",
  editor    = "Koller, D and Schuurmans, D and Bengio, Y and Bottou, L",
  publisher = "Curran Associates, Inc.",
  pages     = "617--624",
  year      =  2009
}



@ARTICLE{Zhao2006-od,
  title    = "On model selection consistency of lasso",
  author   = "Zhao, Peng and Yu, Bin",
  journal  = "J. Mach. Learn. Res.",
  volume   =  7,
  pages    = "2541--2563",
  year     =  2006
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Meinshausen2007-ey,
  title    = "Relaxed Lasso",
  author   = "Meinshausen, Nicolai",
  abstract = "The Lasso is an attractive regularisation method for
              high-dimensional regression. It combines variable selection with
              an efficient computational procedure. However, the rate of
              convergence of the Lasso is slow for some sparse high-dimensional
              data, where the number of predictor variables is growing fast
              with the number of observations. Moreover, many noise variables
              are selected if the estimator is chosen by cross-validation. It
              is shown that the contradicting demands of an efficient
              computational procedure and fast convergence rates of the ?2-loss
              can be overcome by a two-stage procedure, termed the relaxed
              Lasso. For orthogonal designs, the relaxed Lasso provides a
              continuum of solutions that include both soft- and
              hard-thresholding of estimators. The relaxed Lasso solutions
              include all regular Lasso solutions and computation of all
              relaxed Lasso solutions is often identically expensive as
              computing all regular Lasso solutions. Theoretical and numerical
              results demonstrate that the relaxed Lasso produces sparser
              models with equal or lower prediction loss than the regular Lasso
              estimator for high-dimensional data.",
  journal  = "Comput. Stat. Data Anal.",
  volume   =  52,
  number   =  1,
  pages    = "374--393",
  month    =  sep,
  year     =  2007,
  keywords = "High dimensionality; Bridge estimation; Lasso; -norm
              penalisation; Dimensionality reduction"
}


@ARTICLE{Champion2019-lu,
  title    = "Data-driven discovery of coordinates and governing equations",
  author   = "Champion, Kathleen and Lusch, Bethany and Kutz, J Nathan and
              Brunton, Steven L",
  abstract = "The discovery of governing equations from scientific data has the
              potential to transform data-rich fields that lack
              well-characterized quantitative descriptions. Advances in sparse
              regression are currently enabling the tractable identification of
              both the structure and parameters of a nonlinear dynamical system
              from data. The resulting models have the fewest terms necessary
              to describe the dynamics, balancing model complexity with
              descriptive ability, and thus promoting interpretability and
              generalizability. This provides an algorithmic approach to
              Occam's razor for model discovery. However, this approach
              fundamentally relies on an effective coordinate system in which
              the dynamics have a simple representation. In this work, we
              design a custom deep autoencoder network to discover a coordinate
              transformation into a reduced space where the dynamics may be
              sparsely represented. Thus, we simultaneously learn the governing
              equations and the associated coordinate system. We demonstrate
              this approach on several example high-dimensional systems with
              low-dimensional behavior. The resulting modeling framework
              combines the strengths of deep neural networks for flexible
              representation and sparse identification of nonlinear dynamics
              (SINDy) for parsimonious models. This method places the discovery
              of coordinates and models on an equal footing.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  116,
  number   =  45,
  pages    = "22445--22451",
  month    =  nov,
  year     =  2019,
  keywords = "deep learning; dynamical systems; machine learning; model
              discovery",
  language = "en"
}



@ARTICLE{Rudy2019-tk,
  title     = "{Data-Driven} Identification of Parametric Partial Differential
               Equations",
  author    = "Rudy, Samuel and Alla, Alessandro and Brunton, Steven L and
               Kutz, J Nathan",
  abstract  = "In this work we present a data-driven method for the discovery
               of parametric partial differential equations (PDEs), thus
               allowing one to disambiguate between the underlying evolution
               equations and their parametric dependencies. Group sparsity is
               used to ensure parsimonious representations of observed dynamics
               in the form of a parametric PDE, while also allowing the
               coefficients to have arbitrary time series, or spatial
               dependence. This work builds on previous methods for the
               identification of constant coefficient PDEs, expanding the field
               to include a new class of equations which until now have eluded
               machine learning based identification methods. We show that
               group sequentially thresholded ridge regression outperforms
               group LASSO in identifying the fewest terms in the PDE along
               with their parametric dependency. The method is demonstrated on
               four canonical models with and without the introduction of
               noise.",
  journal   = "SIAM J. Appl. Dyn. Syst.",
  publisher = "Society for Industrial and Applied Mathematics",
  volume    =  18,
  number    =  2,
  pages     = "643--660",
  month     =  jan,
  year      =  2019
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wu2010-jj,
  title     = "Learning gradients: predictive models that infer geometry and
               statistical dependence",
  author    = "Wu, Q and Guinney, J and Maggioni, M and Mukherjee, S",
  abstract  = "The problems of dimension reduction and inference of statistical
               dependence are addressed by the modeling framework of learning
               gradients. The models we propose hold for Euclidean spaces as
               well as the manifold setting. The central quantity in this
               approach is an estimate of the gradient of the regression or
               classification function. Two quadratic forms are constructed
               from gradient estimates: the gradient outer product and gradient
               based diffusion maps. The first quantity can be used for
               supervised dimension reduction on manifolds as ?",
  journal   = "J. Mach. Learn. Res.",
  publisher = "jmlr.org",
  year      =  2010
}

@inproceedings{Adebayo2018-rc,
author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
title = {Sanity Checks for Saliency Maps},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {9525–9536},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@ARTICLE{Yang2020-de,
  title         = "Tensor Programs {II}: Neural Tangent Kernel for Any
                   Architecture",
  author        = "Yang, Greg",
  abstract      = "We prove that a randomly initialized neural network of *any
                   architecture* has its Tangent Kernel (NTK) converge to a
                   deterministic limit, as the network widths tend to infinity.
                   We demonstrate how to calculate this limit. In prior
                   literature, the heuristic study of neural network gradients
                   often assumes every weight matrix used in forward
                   propagation is independent from its transpose used in
                   backpropagation (Schoenholz et al. 2017). This is known as
                   the *gradient independence assumption (GIA)*. We identify a
                   commonly satisfied condition, which we call *Simple GIA
                   Check*, such that the NTK limit calculation based on GIA is
                   correct. Conversely, when Simple GIA Check fails, we show
                   GIA can result in wrong answers. Our material here presents
                   the NTK results of Yang (2019a) in a friendly manner and
                   showcases the *tensor programs* technique for understanding
                   wide neural networks. We provide reference implementations
                   of infinite-width NTKs of recurrent neural network,
                   transformer, and batch normalization at
                   https://github.com/thegregyang/NTK4A.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "2006.14548"
}

@inproceedings{Lin2019-zu,
  author={Zinan Lin and Kiran Koshy Thekumparampil and Giulia C. Fanti and Sewoong Oh},
  title={InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs},
  year={2020},
  cdate={1577836800000},
  pages={6127-6139},
  url={http://proceedings.mlr.press/v119/lin20e.html},
  booktitle={ICML},
}

 @inproceedings{Shukla2019-mn,
author = {Shukla, Ankita and Uppal, Shagun and Bhagat, Sarthak and Anand, Saket and Turaga, Pavan},
title = {Geometry of Deep Generative Models for Disentangled Representations},
year = {2018},
isbn = {9781450366151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293353.3293422},
doi = {10.1145/3293353.3293422},
abstract = {Deep generative models like variational autoencoders approximate the intrinsic geometry of high dimensional data manifolds by learning a set of low-dimensional latent-space variables and an embedding function. The geometrical properties of these latent spaces has been studied under the lens of Riemannian geometry; via analysis of the non-linearity of the generator function. In new developments, deep generative models have been used for learning semantically meaningful 'disentangled' representations; that capture task relevant attributes while being invariant to other attributes. In this work, we explore the geometry of popular generative models for disentangled representation learning. We use several metrics to compare the properties of latent spaces of disentangled representation models in terms of class separability and curvature of the latent-space. The proposed study establishes that the class distinguishable features in the disentangled latent space exhibits higher curvature as opposed to a variational autoencoder. We evaluate and compare the geometry of three such models with variational autoencoder on two different datasets. The proposed study shows that the distances and interpolations in the latent space are significantly improved with Riemannian metric owing to the curvature of the space.},
booktitle = {Proceedings of the 11th Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {68},
numpages = {8},
keywords = {Deep generative models, Disentangled representations, Riemannian Geometry},
location = {Hyderabad, India},
series = {ICVGIP 2018}
}

@ARTICLE{Amir2013-px,
  title    = "{viSNE} enables visualization of high dimensional single-cell
              data and reveals phenotypic heterogeneity of leukemia",
  author   = "Amir, El-Ad David and Davis, Kara L and Tadmor, Michelle D and
              Simonds, Erin F and Levine, Jacob H and Bendall, Sean C and
              Shenfeld, Daniel K and Krishnaswamy, Smita and Nolan, Garry P and
              Pe'er, Dana",
  abstract = "New high-dimensional, single-cell technologies offer
              unprecedented resolution in the analysis of heterogeneous
              tissues. However, because these technologies can measure dozens
              of parameters simultaneously in individual cells, data
              interpretation can be challenging. Here we present viSNE, a tool
              that allows one to map high-dimensional cytometry data onto two
              dimensions, yet conserve the high-dimensional structure of the
              data. viSNE plots individual cells in a visual similar to a
              scatter plot, while using all pairwise distances in high
              dimension to determine each cell's location in the plot. We
              integrated mass cytometry with viSNE to map healthy and cancerous
              bone marrow samples. Healthy bone marrow automatically maps into
              a consistent shape, whereas leukemia samples map into malformed
              shapes that are distinct from healthy bone marrow and from each
              other. We also use viSNE and mass cytometry to compare leukemia
              diagnosis and relapse samples, and to identify a rare leukemia
              population reminiscent of minimal residual disease. viSNE can be
              applied to any multi-dimensional single-cell technology.",
  journal  = "Nat. Biotechnol.",
  volume   =  31,
  number   =  6,
  pages    = "545--552",
  month    =  jun,
  year     =  2013,
  language = "en"
}


@ARTICLE{Fan2001-nv,
  title     = "Variable selection via nonconcave penalized likelihood and its
               oracle properties",
  author    = "Fan, Jianqing and Li, Runze",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Informa UK Limited",
  volume    =  96,
  number    =  456,
  pages     = "1348--1360",
  month     =  dec,
  year      =  2001
}

@ARTICLE{Efron2004-eb,
  title     = "Least angle regression",
  author    = "Efron, Bradley and Hastie, Trevor and Johnstone, Iain and
               Tibshirani, Robert and {Others}",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  32,
  number    =  2,
  pages     = "407--499",
  year      =  2004
}


@ARTICLE{Zou2006-bj,
  title     = "The Adaptive Lasso and Its Oracle Properties",
  author    = "Zou, Hui",
  abstract  = "The lasso is a popular technique for simultaneous estimation and
               variable selection. Lasso variable selection has been shown to
               be consistent under certain conditions. In this work we derive a
               necessary condition for the lasso variable selection to be
               consistent. Consequently, there exist certain scenarios where
               the lasso is inconsistent for variable selection. We then
               propose a new version of the lasso, called the adaptive lasso,
               where adaptive weights are used for penalizing different
               coefficients in the ?1 penalty. We show that the adaptive lasso
               enjoys the oracle properties; namely, it performs as well as if
               the true underlying model were given in advance. Similar to the
               lasso, the adaptive lasso is shown to be near-minimax optimal.
               Furthermore, the adaptive lasso can be solved by the same
               efficient algorithm for solving the lasso. We also discuss the
               extension of the adaptive lasso in generalized linear models and
               show that the oracle properties still hold under mild regularity
               conditions. As a byproduct of our theory, the nonnegative
               garotte is shown to be consistent for variable selection.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  101,
  number    =  476,
  pages     = "1418--1429",
  month     =  dec,
  year      =  2006
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Huan_Xu2012-iu,
  title    = "Sparse Algorithms Are Not Stable: A {No-Free-Lunch} Theorem",
  author   = "{Huan Xu} and Caramanis, C and Mannor, S",
  abstract = "We consider two desired properties of learning algorithms:
              sparsity and algorithmic stability. Both properties are believed
              to lead to good generalization ability. We show that these two
              properties are fundamentally at odds with each other: A sparse
              algorithm cannot be stable and vice versa. Thus, one has to trade
              off sparsity and stability in designing a learning algorithm. In
              particular, our general result implies that ?(1)-regularized
              regression (Lasso) cannot be stable, while ?(2)-regularized
              regression is known to have strong stability properties and is
              therefore not sparse.",
  journal  = "IEEE Trans. Pattern Anal. Mach. Intell.",
  volume   =  34,
  number   =  1,
  pages    = "187--193",
  month    =  jan,
  year     =  2012,
  language = "en"
}

@article{buitinck2013,
  title={{API} design for machine learning software: experiences from the scikit-learn project},
  author={Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and others},
  journal={arXiv preprint arXiv:1309.0238},
  year={2013}
}

@article{Chen2001-hh,
  title    = "Atomic Decomposition by Basis Pursuit",
  author   = {{Chen, Scott S} and {Donoho, David L} and {Saunders, Michael A}},
  abstract = "The time-frequency and time-scale communities have recently
              developed a large number of overcomplete waveform
              dictionaries---stationary wavelets, wavelet packets, cosine
              packets, chirplets, and warplets, to name a few. Decomposition
              into overcomplete systems is not unique, and several methods for
              decomposition have been proposed, including the method of frames
              (MOF), matching pursuit (MP), and, for special dictionaries, the
              best orthogonal basis (BOB). Basis pursuit (BP) is a principle
              for decomposing a signal into an ``optimal'' superposition of
              dictionary elements, where optimal means having the smallest l 1
              norm of coefficients among all such decompositions. We give
              examples exhibiting several advantages over MOF, MP, and BOB,
              including better sparsity and superresolution. BP has interesting
              relations to ideas in areas as diverse as ill-posed problems,
              abstract harmonic analysis, total variation denoising, and
              multiscale edge denoising. BP in highly overcomplete dictionaries
              leads to large-scale optimization problems. With signals of
              length 8192 and a wavelet packet dictionary, one gets an
              equivalent linear program of size 8192 by 212,992. Such problems
              can be attacked successfully only because of recent advances in
              linear and quadratic programming by interior-point methods. We
              obtain reasonable success with a primal-dual logarithmic barrier
              method and conjugategradient solver.",
  journal  = "SIAM REVIEW",
  volume   =  43,
  number   =  1,
  pages    = "129",
  month    =  feb,
  year     =  2001
}


@book{Hastie2015-sl,
	author = {Trevor Hastie and Robert Tibshirani},
	title = {Statistical learning with sparsity : the lasso and generalizations},
	publisher = {CRC Press},
	year = {2015},
	series = {Monographs on statistics and applied probability, no. 143},
	edition = {Special Indian ed.}
}

@ARTICLE{Hesterberg2008-hm,
author = {Tim Hesterberg and Nam Hee Choi and Lukas Meier and Chris Fraley},
title = {{Least angle and $\ell_1$ penalized regression: A review}},
volume = {2},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {61 -- 93},
keywords = {L1 penalty, Lasso, regression, regularization, Variable selection},
year = {2008},
doi = {10.1214/08-SS035},
URL = {https://doi.org/10.1214/08-SS035}
}


@ARTICLE{Brunton2016-dt,
  title    = "Discovering governing equations from data by sparse
              identification of nonlinear dynamical systems",
  author   = "Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan",
  abstract = "Extracting governing equations from data is a central challenge
              in many diverse areas of science and engineering. Data are
              abundant whereas models often remain elusive, as in climate
              science, neuroscience, ecology, finance, and epidemiology, to
              name only a few examples. In this work, we combine
              sparsity-promoting techniques and machine learning with nonlinear
              dynamical systems to discover governing equations from noisy
              measurement data. The only assumption about the structure of the
              model is that there are only a few important terms that govern
              the dynamics, so that the equations are sparse in the space of
              possible functions; this assumption holds for many physical
              systems in an appropriate basis. In particular, we use sparse
              regression to determine the fewest terms in the dynamic governing
              equations required to accurately represent the data. This results
              in parsimonious models that balance accuracy with model
              complexity to avoid overfitting. We demonstrate the algorithm on
              a wide range of problems, from simple canonical systems,
              including linear and nonlinear oscillators and the chaotic Lorenz
              system, to the fluid vortex shedding behind an obstacle. The
              fluid example illustrates the ability of this method to discover
              the underlying dynamics of a system that took experts in the
              community nearly 30 years to resolve. We also show that this
              method generalizes to parameterized systems and systems that are
              time-varying or have external forcing.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  113,
  number   =  15,
  pages    = "3932--3937",
  month    =  apr,
  year     =  2016,
  keywords = "dynamical systems; machine learning; optimization; sparse
              regression; system identification",
  language = "en"
}




@Article{carilllo:08,
  author = 	 {Carrillo-Reid, L. et al.},
  title = 	 {Encoding network states by striatal cell assemblies},
  journal = 	 {Journal of Neurophysiology},
  year = 	 {2008},
  volume = 	 {99},
  number = 	 {},
  pages = 	 {1435--1450},
  note = 	 {uses isomap/lle},
}

@Article{casettiClementiPettini:96,
  author = 	 {L. Casetti and C. Clementi and M. Pettini},
  title = 	 {Riemannian theory of {H}amiltonian chaos and {L}yapunov exponents},
  journal = 	 {Physical Review E},
  year = 	 {1996},
  key = 	 {},
  volume = 	 {54},
  number = 	 {6},
  pages = 	 {5969},
  note = 	 {},
  annote = 	 {}
}



@Article{ChaudhuriRichardsonRobinsZivot:journal-07,
  author =   {Saraswata Chaudhuri and Thomas Richardson and James Robins and Eric Zivot},
  title =    {Split-Sample Score Tests in Linear Instrumental Variables Regression},
  journal =    {Econometric Theory},
  year =   {2010},
 volume =    {18},
 number =    {},
 pages =   {},
 note =    {(The paper was also presented at the Summer Meeting of the Econometric Society, 2007)},
 annote =    {doi:10.1017/S0266466609990806},
 url = {http://journals.cambridge.org/action/displayIssue?iid=440399}
}

@article{lobpcg:2001,
	author = {Andrew V. Knyazev},
	title = {Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method},
	journal = {SIAM Journal on Scientific Computing},
	volume = {23},
	number = {2},
	pages = {517-541},
	year = {2001},
	doi = {10.1137/S1064827500366124},
	URL = {http://dx.doi.org/10.1137/S1064827500366124}
}


@InProceedings{mikolov:13word2vec,
  author = 	 {Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
  title = 	 {Distributed Representations of Words and Phrases and their Compositionality},
 booktitle = {Advances in Neural Information Processing Systems 26},
  pages = 	 {},
  year = 	 {2013},
  editor = 	 {},
  address = 	 {},
  publisher = {},
}


% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Candes2007-ie,
  title     = "The Dantzig selector: Statistical estimation when p is much
               larger than n",
  author    = "Candes, Emmanuel and Tao, Terence",
  abstract  = "In many important statistical applications, the number of
               variables or parameters p is much larger than the number of
               observations n. Suppose then that we have observations
               y=X$\beta$+z, where $\beta$$\in$Rp is a parameter vector of
               interest, X is a data matrix with possibly far fewer rows than
               columns, n?p, and the zi's are i.i.d. N(0, $\sigma$2). Is it
               possible to estimate $\beta$ reliably based on the noisy data
               y?To estimate $\beta$, we introduce a new estimator---we call it
               the Dantzig selector---which is a solution to the
               ?1-regularization problem
               $$\textbackslashmin_\{\textbackslashtilde\{\textbackslashbeta\}\textbackslashin\textbackslashmathbf\{R\}^\{p\}\}\textbackslash|\textbackslashtilde\{\textbackslashbeta\}\textbackslash|_\{\textbackslashell_\{1\}\}\textbackslashquad\textbackslashmbox\{subject
               to\}\textbackslashquad
               \textbackslash|X^\{*\}r\textbackslash|_\{\textbackslashell_\{\textbackslashinfty\}\}\textbackslashleq(1+t^\{-1\})\textbackslashsqrt\{2\textbackslashlog
               p\}\textbackslashcdot\textbackslashsigma,$$ where r is the
               residual vector y?X$\beta? and t is a positive scalar. We show
               that if X obeys a uniform uncertainty principle (with
               unit-normed columns) and if the true parameter vector $\beta$ is
               sufficiently sparse (which here roughly guarantees that the
               model is identifiable), then with very large
               probability,?$\beta??$\beta$??22$\leq$C2$\cdot$2log
               p$\cdot$($\sigma$2+$\sum$imin($\beta$i2, $\sigma$2)).Our results
               are nonasymptotic and we give values for the constant C. Even
               though n may be much smaller than p, our estimator achieves a
               loss within a logarithmic factor of the ideal mean squared error
               one would achieve with an oracle which would supply perfect
               information about which coordinates are nonzero, and which were
               above the noise level.In multivariate regression and from a
               model selection viewpoint, our result says that it is possible
               nearly to select the best subset of variables by solving a very
               simple convex program, which, in fact, can easily be recast as a
               convenient linear program (LP).",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  35,
  number    =  6,
  pages     = "2313--2351",
  month     =  dec,
  year      =  2007,
  keywords  = "62C05; 62G05; 94A08; 94A12; geometry in high dimensions; ideal
               estimation; ?\_1-minimization; linear programming; Model
               selection; Oracle inequalities; random matrices; restricted
               orthonormality; sparse solutions to underdetermined systems;
               Statistical linear model;",
  language  = "en"
}


@ARTICLE{Meinshausen2010-nr,
  title     = "Stability selection: Stability selection",
  author    = "Meinshausen, Nicolai and B{\"u}hlmann, Peter",
  abstract  = "Summary.? Estimation of structure, such as in variable
               selection, graphical modelling or cluster analysis, is
               notoriously difficult, especially for high dimensional data. We
               introduce stability selection. It is based on subsampling in
               combination with (high dimensional) selection algorithms. As
               such, the method is extremely general and has a very wide range
               of applicability. Stability selection provides finite sample
               control for some error rates of false discoveries and hence a
               transparent principle to choose a proper amount of
               regularization for structure estimation. Variable selection and
               structure estimation improve markedly for a range of selection
               methods if stability selection is applied. We prove for the
               randomized lasso that stability selection will be variable
               selection consistent even if the necessary conditions for
               consistency of the original lasso method are violated. We
               demonstrate stability selection for variable selection and
               Gaussian graphical modelling, using real and simulated data.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  72,
  number    =  4,
  pages     = "417--473",
  month     =  jul,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Lederer2015-pr,
  title         = "Don't fall for tuning parameters: Tuning-free variable
                   selection in high dimensions with the {TREX}",
  author        = "Johannes Lederer and Christian M{\"u}ller",
  abstract      = "Lasso is a seminal contribution to high-dimensional
                   statistics, but it hinges on a tuning parameter that is
                   difficult to calibrate in practice. A partial remedy for
                   this problem is Square-Root Lasso, because it inherently
                   calibrates to the noise variance. However, Square-Root Lasso
                   still requires the calibration of a tuning parameter to all
                   other aspects of the model. In this study, we introduce
                   TREX, an alternative to Lasso with an inherent calibration
                   to all aspects of the model. This adaptation to the entire
                   model renders TREX an estimator that does not require any
                   calibration of tuning parameters. We show that TREX can
                   outperform cross-validated Lasso in terms of variable
                   selection and computational efficiency. We also introduce a
                   bootstrapped version of TREX that can further improve
                   variable selection. We illustrate the promising performance
                   of TREX both on synthetic data and on a recent
                   high-dimensional biological data set that considers
                   riboflavin production in B. subtilis.",
  journal       = "Proceedings of the Twenty-Ninth AAAI Conference on
                   Artificial Intelligenc",
  month         =  jan,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1404.0541"
}

@ARTICLE{Meinshausen2008-ez,
author = {Nicolai Meinshausen and Bin Yu},
title = {{Lasso-type recovery of sparse representations for high-dimensional data}},
volume = {37},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {246 -- 270},
keywords = {High-dimensional data, Lasso, shrinkage estimation, Sparsity},
year = {2009},
doi = {10.1214/07-AOS582},
URL = {https://doi.org/10.1214/07-AOS582}
}



 @InProceedings{RichardsonZivotChaudhuri:esrc07,
  author = 	 {Thomas Richardson and Eric Zivot and Saraswata Chaudhuri},
  title = 	 {A new Score-type test for subsets of Parameters},
  booktitle = {Winter Meeting of the Econometric Society},
  pages = 	 {},
  year = 	 {2008},
  editor = 	 {},
  volume = 	 {},
  address = 	 {},
  publisher = {},
  note = 	 {(An early version of this paper was presented at the Annual meeting of the ESRC 
Study Group in Bristol, July, 2007)},
}
% ----- end  ITR02 grant, Richardson ---------------



@Article{saha:13,
  author =   {Saha, D. et al. },
  title =    {A spatiotemporal coding mechanism for background-invariant odor recognition},
  journal =    {Nature Neuroscience},
  year =   {2013},
  note =   {uses isomap/lle},
}

@ARTICLE{Yuan2006-pa,
  title     = "Model selection and estimation in regression with grouped
               variables",
  author    = "Yuan, M and Lin, Y",
  abstract  = "We consider the problem of selecting grouped variables (factors)
               for accurate prediction in regression. Such a problem arises
               naturally in many practical situations with the multifactor
               analysis‐of‐variance problem as the most important and
               well‐known example. Instead of …",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley Online Library",
  year      =  2006
}

@ARTICLE{Yip2004,
   author = {{Yip}, C.~W. and {Connolly}, A.~J. and {Szalay}, A.~S. and {Budav{\'a}ri}, T. and
  {SubbaRao}, M. and {Frieman}, J.~A. and {Nichol}, R.~C. and
  {Hopkins}, A.~M. and {York}, D.~G. and {Okamura}, S. and {Brinkmann}, J. and
  {Csabai}, I. and {Thakar}, A.~R. and {Fukugita}, M. and {Ivezi{\'c}}, {\v Z}.
  },
    title = "{Distributions of Galaxy Spectral Types in the Sloan Digital Sky Survey}",
  journal = {Astronomical Journal},
   eprint = {astro-ph/0407061},
 keywords = {Galaxies: Fundamental Parameters, Galaxies: General, Methods: Data Analysis, Techniques: Spectroscopic},
     year = 2004,
    month = aug,
   volume = 128,
    pages = {585-609},
      doi = {10.1086/422429},
   adsurl = {http://adsabs.harvard.edu/abs/2004AJ....128..585Y},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{SingularPertTonyCai2018,
  title={Rate-optimal perturbation bounds for singular subspaces with applications to high-dimensional statistics},
  author={T. Tony Cai and Anru Zhang},
  booktitle={The Annals of Statistics},
  volume={46},
  year={2018}
}
